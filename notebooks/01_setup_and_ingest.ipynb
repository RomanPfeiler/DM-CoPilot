{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8028bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pfeil\\anaconda3\\envs\\dm_copilot\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# notebooks/01_setup_and_ingest.ipynb\n",
    "\n",
    "# %% [markdown]\n",
    "# # 01. Setup and Knowledge Ingestion\n",
    "# \n",
    "# ## Goal\n",
    "# We will load two PDF sources (Rules and Campaign), chunk them, tag them with metadata, and save them to a Vector Database.\n",
    "#\n",
    "# ## Prerequisites\n",
    "# * Ensure you have `dnd_rules.pdf` AND `campaign.pdf` in `data/raw/`. \n",
    "# * (If you only have one file, just comment out the second loader code).\n",
    "\n",
    "# %%\n",
    "# 1. Imports\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Add src to path to import config\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "from config import RAW_DATA_DIR, VECTOR_STORE_DIR, EMBEDDING_MODEL_PATH\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "220d8e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Rules: c:\\Users\\pfeil\\My Drive\\Studys\\CAS-NLP-Uni-Bern\\Module 4  NLP Transformers\\dungeon_master_copilot\\data\\raw\\dnd_rules.pdf\n",
      "Loading Campaign: c:\\Users\\pfeil\\My Drive\\Studys\\CAS-NLP-Uni-Bern\\Module 4  NLP Transformers\\dungeon_master_copilot\\data\\raw\\campaign.pdf\n",
      "✅ Total Pages Loaded: 850\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Step 1: Load PDFs and Assign Metadata\n",
    "# We need to label the data so we can later ask: \"What did the Campaign book say?\" vs \"What did the Rule book say?\"\n",
    "\n",
    "# %%\n",
    "# Define your filenames here\n",
    "RULE_PDF = os.path.join(RAW_DATA_DIR, \"dnd_rules.pdf\")\n",
    "CAMPAIGN_PDF = os.path.join(RAW_DATA_DIR, \"campaign.pdf\")\n",
    "\n",
    "documents = []\n",
    "\n",
    "# --- Load Rules ---\n",
    "if os.path.exists(RULE_PDF):\n",
    "    print(f\"Loading Rules: {RULE_PDF}\")\n",
    "    rule_loader = PyPDFLoader(RULE_PDF)\n",
    "    rule_docs = rule_loader.load()\n",
    "    # Add Metadata Tag\n",
    "    for doc in rule_docs:\n",
    "        doc.metadata[\"source_type\"] = \"rulebook\"\n",
    "    documents.extend(rule_docs)\n",
    "else:\n",
    "    print(f\"⚠️ Warning: {RULE_PDF} not found.\")\n",
    "\n",
    "# --- Load Campaign ---\n",
    "if os.path.exists(CAMPAIGN_PDF):\n",
    "    print(f\"Loading Campaign: {CAMPAIGN_PDF}\")\n",
    "    camp_loader = PyPDFLoader(CAMPAIGN_PDF)\n",
    "    camp_docs = camp_loader.load()\n",
    "    # Add Metadata Tag\n",
    "    for doc in camp_docs:\n",
    "        doc.metadata[\"source_type\"] = \"campaign\"\n",
    "    documents.extend(camp_docs)\n",
    "else:\n",
    "    print(f\"⚠️ Warning: {CAMPAIGN_PDF} not found.\")\n",
    "\n",
    "print(f\"✅ Total Pages Loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a1a57c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 1444 chunks.\n",
      "Example Metadata: {'producer': 'WeasyPrint 58.1', 'creator': 'pandoc', 'creationdate': '', 'title': 'System Reference Document 5.1', 'source': 'c:\\\\Users\\\\pfeil\\\\My Drive\\\\Studys\\\\CAS-NLP-Uni-Bern\\\\Module 4  NLP Transformers\\\\dungeon_master_copilot\\\\data\\\\raw\\\\dnd_rules.pdf', 'total_pages': 818, 'page': 0, 'page_label': '1', 'source_type': 'rulebook'}\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Step 2: Split Text into Chunks\n",
    "\n",
    "# %%\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=300,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(documents)\n",
    "print(f\"Created {len(splits)} chunks.\")\n",
    "\n",
    "# Verify metadata is preserved\n",
    "if len(splits) > 0:\n",
    "    print(f\"Example Metadata: {splits[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e684666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Embedding Model from: c:\\Users\\pfeil\\My Drive\\Studys\\CAS-NLP-Uni-Bern\\Module 4  NLP Transformers\\dungeon_master_copilot\\Output\\fine_tuned_qwen_dnd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pfeil\\AppData\\Local\\Temp\\ipykernel_11484\\493808289.py:33: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(\n",
      "The tokenizer you are loading from 'c:\\Users\\pfeil\\My Drive\\Studys\\CAS-NLP-Uni-Bern\\Module 4  NLP Transformers\\dungeon_master_copilot\\Output\\fine_tuned_qwen_dnd' with an incorrect regex pattern: https://huggingface.co/mistralai/Mistral-Small-3.1-24B-Instruct-2503/discussions/84#69121093e8b480e709447d5e. This will lead to incorrect tokenization. You should set the `fix_mistral_regex=True` flag when loading this tokenizer to fix this issue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully.\n",
      "Attempting to clear old Vector Store...\n",
      "⚠️ Could not fully delete folder: [WinError 5] Access is denied: 'c:\\\\Users\\\\pfeil\\\\My Drive\\\\Studys\\\\CAS-NLP-Uni-Bern\\\\Module 4  NLP Transformers\\\\dungeon_master_copilot\\\\data\\\\vector_store'\n",
      "   (This is common with Google Drive. We will try to overwrite anyway.)\n",
      "Creating new Vector Store (this may take time)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 46/46 [44:45<00:00, 58.38s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector Store successfully saved to c:\\Users\\pfeil\\My Drive\\Studys\\CAS-NLP-Uni-Bern\\Module 4  NLP Transformers\\dungeon_master_copilot\\data\\vector_store\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Step 3: Initialize Embeddings and Create Vector Store\n",
    "# **Note:** If a vector store already exists, we clear it first to avoid duplicate data during testing.\n",
    "\n",
    "# %%\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "\n",
    "# Define a helper to force-delete read-only files (common on Windows)\n",
    "def on_error(func, path, exc_info):\n",
    "    import stat\n",
    "    if not os.access(path, os.W_OK):\n",
    "        os.chmod(path, stat.S_IWUSR)\n",
    "        func(path)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "# Initialize Local Fine-Tuned Embeddings\n",
    "print(f\"Loading Embedding Model from: {EMBEDDING_MODEL_PATH}\")\n",
    "\n",
    "# Check if the folder exists (Critical for local models)\n",
    "if os.path.isabs(EMBEDDING_MODEL_PATH) and not os.path.exists(EMBEDDING_MODEL_PATH):\n",
    "    raise FileNotFoundError(\n",
    "        f\"❌ Could not find model at {EMBEDDING_MODEL_PATH}.\\n\"\n",
    "    )\n",
    "\n",
    "model_kwargs = {\n",
    "    \"trust_remote_code\": True, \n",
    "    \"device\": \"cpu\"\n",
    "}\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_PATH,\n",
    "    model_kwargs=model_kwargs,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"✅ Model loaded successfully.\")\n",
    "\n",
    "# Clear old DB if exists (Fresh Start)\n",
    "if os.path.exists(VECTOR_STORE_DIR):\n",
    "    print(\"Attempting to clear old Vector Store...\")\n",
    "    \n",
    "    # 1. Force Python to release file handles\n",
    "    # If vectorstore existed in memory from a previous run, this kills it.\n",
    "    if 'vectorstore' in globals():\n",
    "        del vectorstore\n",
    "    gc.collect() \n",
    "    \n",
    "    # 2. Try to delete with retries\n",
    "    try:\n",
    "        shutil.rmtree(VECTOR_STORE_DIR, onerror=on_error)\n",
    "        print(\"Cleared old Vector Store.\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not fully delete folder: {e}\")\n",
    "        print(\"   (This is common with Google Drive. We will try to overwrite anyway.)\")\n",
    "\n",
    "# Create and Save\n",
    "print(\"Creating new Vector Store (this may take time)...\")\n",
    "# Add a small sleep to let the OS catch up\n",
    "time.sleep(1)\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=VECTOR_STORE_DIR\n",
    ")\n",
    "\n",
    "print(f\"✅ Vector Store successfully saved to {VECTOR_STORE_DIR}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm_copilot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
